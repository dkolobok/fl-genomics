experiment:
  description: MLP model
  name: federated_mlp_standing-height
  missing: zero
  random_state: 4
server:
  rounds: 16
  strategy:
    name: fedavg
    node_count: ${env:FL_NODE_COUNT,2}
  
  # strategy:
  #   name: qfedavg
  #   node_count: ${env:FL_NODE_COUNT,2}
  #   args: 
  #     q_param: 0.2
  #     qffl_learning_rate: 0.1

  # strategy:
  #   name: fedadam
  #   node_count: ${env:FL_NODE_COUNT,2}
  #   args:
  #     eta: 1e-1
  #     eta_l: 1e-1
  #     beta_1: 0.9
  #     beta_2: 0.99
  #     tau: 1e-9 

  # strategy:
  #   name: fedadagrad
  #   node_count: ${env:FL_NODE_COUNT,2}
  #   args:
  #     eta: 1e-1
  #     eta_l: 1e-1
  #     tau: 1e-9  

node:
  model:
    name: mlp_regressor
    batch_size: 256
    l1: 0.0
    hidden_size: 4096
  optimizer:
    name: adamw
    lr: 1e-4
    weight_decay: 1e-3
  scheduler:
    name: one_cycle_lr
    rounds: 16
    epochs_in_round: 4
    div_factor: 25
    final_div_factor: 1e+4
  training:
    gpus:
    - 0
    max_epochs: ${node.scheduler.epochs_in_round}
    enable_progress_bar: False
    enable_model_summary: False
